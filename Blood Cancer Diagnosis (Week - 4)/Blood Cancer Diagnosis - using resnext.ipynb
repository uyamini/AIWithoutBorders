{"cells":[{"metadata":{},"cell_type":"markdown","source":"Aim:\nFinding a suitable model for classification of leukemic B-lymphoblast cells from normal B-lymphoid precursors from blood smear microscopic images. \n\nWe have used a CNN model and some pretrained models like ResNet50, VGG16, VGG19 and InceptionV3 in this process.\nAfter training the models for 15 epochs, the VGG19 model came up with the highest accuracy among all of them with about ~78% accuracy and VGG16 was the second highest with a little less accuracy than the former with about ~77%.\n\nThe reason behind VGG models getting so much higher accuracy in comparison to others is its architecture, the main key points of this architecture are as follows: \n\n*    Use of very small convolutional filters, e.g. 3×3 and 1×1 with a stride of one.\n*    Use of max pooling with a size of 2×2 and a stride of the same dimensions.\n*    The importance of stacking convolutional layers together before using a pooling layer to define a block.\n*    Dramatic repetition of the convolutional-pooling block pattern.\n*    Development of very deep (16 and 19 layer) models.\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys\nprint(os.listdir(\"../input\"))\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (10, 5)\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['blood-cancer-test-dataset', 'blood-cancer-training-dataset']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom glob import glob\n%matplotlib inline\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import confusion_matrix\n\nimport keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, DepthwiseConv2D\nfrom keras import backend as K\nimport itertools\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"os.listdir(\"../input\")","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"['blood-cancer-test-dataset', 'blood-cancer-training-dataset']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell can be used to access and use the TEST dataset available\n\n# data = pd.read_csv(\"../input/blood-cancer-test-dataset/c-nmc_test_prelim_phase_data/C-NMC_test_prelim_phase_data/C-NMC_test_prelim_phase_data_labels.csv\")\n# print(data.head())\n\n# baseadd = \"../input/blood-cancer-test-dataset/c-nmc_test_prelim_phase_data/C-NMC_test_prelim_phase_data/C-NMC_test_prelim_phase_data\"\n# image_path_dict = {os.path.basename(x): x for x in glob(os.path.join(baseadd, \"*.bmp\"))}\n# data[\"path\"] = (data[\"new_names\"]).map(image_path_dict.get)\n# data['image'] = data['path'].map(lambda x: np.asarray(Image.open(x).resize((224, 224))))\n# data[\"image\"].shape\n# print(data.head())\n\n# data[\"Patient_ID\"][0].split('_')\n# subject_id_dict = {data[\"Patient_ID\"][x]: data[\"Patient_ID\"][x].split(\"_\")[1] for x in range(len(data[\"Patient_ID\"]))}\n# image_id_dict = {data[\"Patient_ID\"][x]: data[\"Patient_ID\"][x].split(\"_\")[2] for x in range(len(data[\"Patient_ID\"]))}\n# cell_id_dict = {data[\"Patient_ID\"][x]: data[\"Patient_ID\"][x].split(\"_\")[3] for x in range(len(data[\"Patient_ID\"]))}\n\n# data[\"subject_id\"] = data[\"Patient_ID\"].map(subject_id_dict.get)\n# data[\"image_id\"] = data[\"Patient_ID\"].map(image_id_dict.get)\n# data[\"cell_no\"] = data[\"Patient_ID\"].map(cell_id_dict.get)\n# # data[\"cell_state\"] = data[\"Patient_ID\"].map(cell_type_id_dict.get)\n# subject_filter_dict = {data[\"subject_id\"][x]: data[\"subject_id\"][x][1:] if \"H\" in data[\"subject_id\"][x] \n#                        else data[\"subject_id\"][x] for x in range(len(data[\"Patient_ID\"]))}\n# data[\"subject_id\"] = data[\"subject_id\"].map(subject_filter_dict.get)\n# print(data.sample(5))\n\n# plt.imshow(data[\"image\"][0])\n# plt.axis(\"off\")\n\n# data[\"labels\"].value_counts().plot(kind = \"bar\")\n# print(data[\"labels\"].value_counts())\n\n# plt.scatter(data['subject_id'], data['cell_no'])\n# plt.title(\"Subject Id distribution\")\n\n# data['subject_id'].value_counts().plot(kind = 'bar')\n# plt.title(\"Subject Id dis\")","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"baseadd_tr = \"../input/blood-cancer-training-dataset/c-nmc_training_data(1)/C-NMC_training_data\"\nimage_path_dict_tr = {os.path.basename(x).split(\".\")[0]: x for x in glob(os.path.join(baseadd_tr, \"*\", \"*\", \"*.bmp\"))}","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr = pd.DataFrame()\npassword = [os.path.basename(x).split(\".\")[0] for x in glob(os.path.join(baseadd_tr, \"*\", \"*\", \"*.bmp\"))]\ndata_tr[\"Patient_ID\"] = password\ndata_tr.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"          Patient_ID\n0   UID_H14_26_1_hem\n1   UID_H10_63_1_hem\n2   UID_H14_30_6_hem\n3  UID_H10_190_3_hem\n4  UID_H14_19_15_hem","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UID_H14_26_1_hem</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UID_H10_63_1_hem</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UID_H14_30_6_hem</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UID_H10_190_3_hem</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UID_H14_19_15_hem</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr[\"path\"] = data_tr[\"Patient_ID\"].map(image_path_dict_tr.get)\ndata_tr['image'] = data_tr['path'].map(lambda x: np.asarray(Image.open(x).resize((100, 100))))\ndata_tr.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"          Patient_ID                        ...                                                                      image\n0   UID_H14_26_1_hem                        ...                          [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n1   UID_H10_63_1_hem                        ...                          [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n2   UID_H14_30_6_hem                        ...                          [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n3  UID_H10_190_3_hem                        ...                          [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n4  UID_H14_19_15_hem                        ...                          [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_ID</th>\n      <th>path</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UID_H14_26_1_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UID_H10_63_1_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UID_H14_30_6_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UID_H10_190_3_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UID_H14_19_15_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr[\"image\"].shape","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"(10661,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr[\"Patient_ID\"][0].split('_')\nsubject_id_dict_tr = {data_tr[\"Patient_ID\"][x]: data_tr[\"Patient_ID\"][x].split(\"_\")[1] for x in range(len(data_tr[\"Patient_ID\"]))}\nimage_id_dict_tr = {data_tr[\"Patient_ID\"][x]: data_tr[\"Patient_ID\"][x].split(\"_\")[2] for x in range(len(data_tr[\"Patient_ID\"]))}\ncell_id_dict_tr = {data_tr[\"Patient_ID\"][x]: data_tr[\"Patient_ID\"][x].split(\"_\")[3] for x in range(len(data_tr[\"Patient_ID\"]))}\ndata_tr[\"subject_id\"] = data_tr[\"Patient_ID\"].map(subject_id_dict_tr.get)\ndata_tr[\"image_id\"] = data_tr[\"Patient_ID\"].map(image_id_dict_tr.get)\ndata_tr[\"cell_no\"] = data_tr[\"Patient_ID\"].map(cell_id_dict_tr.get)\n# data[\"cell_state\"] = data[\"Patient_ID\"].map(cell_type_id_dict.get)\nsubject_filter_dict_tr = {data_tr[\"subject_id\"][x]: data_tr[\"subject_id\"][x][1:] if (\"H\") in data_tr[\"subject_id\"][x] \n                       else data_tr[\"subject_id\"][x] for x in range(len(data_tr[\"Patient_ID\"]))}\ndata_tr[\"subject_id\"] = data_tr[\"subject_id\"].map(subject_filter_dict_tr.get)\ndata_tr.sample(5)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"            Patient_ID   ...   cell_no\n9116  UID_45_35_10_all   ...        10\n4498   UID_H2_34_4_hem   ...         4\n7107   UID_24_35_2_all   ...         2\n6311   UID_47_12_4_all   ...         4\n9503    UID_4_12_2_all   ...         2\n\n[5 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_ID</th>\n      <th>path</th>\n      <th>image</th>\n      <th>subject_id</th>\n      <th>image_id</th>\n      <th>cell_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9116</th>\n      <td>UID_45_35_10_all</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>45</td>\n      <td>35</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4498</th>\n      <td>UID_H2_34_4_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>2</td>\n      <td>34</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7107</th>\n      <td>UID_24_35_2_all</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>24</td>\n      <td>35</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6311</th>\n      <td>UID_47_12_4_all</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>47</td>\n      <td>12</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9503</th>\n      <td>UID_4_12_2_all</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>4</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 means normal cell, while 1 means a cancer diagnosed cell\nlabel_dict = {password[x] : password[x].split(\"_\")[-1] for x in range(len(password))}\ndata_tr[\"labels\"] = data_tr[\"Patient_ID\"].map(label_dict.get)\ndata_tr[\"labels\"] = data_tr[\"labels\"].apply(lambda x: 1 if x == 'all' else 0).astype(np.bool)\ndata_tr[\"subject_id\"] = data_tr[\"subject_id\"].apply(lambda x: \"3\" if x == \"h3\" else x)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting dicts and list whose work is done in order to free our memory.\ndel image_path_dict_tr\ndel password\ndel subject_id_dict_tr\ndel image_id_dict_tr\ndel cell_id_dict_tr\ndel subject_filter_dict_tr\ndel baseadd_tr\ndel label_dict","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plt.imshow(data_tr[\"image\"][4])\nplt.axis(\"off\")","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(-0.5, 99.5, 99.5, -0.5)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr[\"labels\"].value_counts().plot(kind = \"bar\")\ndata_tr[\"labels\"].value_counts()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"True     7272\nFalse    3389\nName: labels, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr['subject_id'].value_counts().plot(kind = 'bar')\nplt.title(\"Subject Id distribution\")\nplt.figure(figsize = (60, 60))","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<Figure size 4320x4320 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 4320x4320 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr[\"cell_no\"] = data_tr[\"cell_no\"].astype(np.int32)\ndata_tr[\"subject_id\"] = data_tr[\"subject_id\"].astype(np.int32)\ndata_tr[\"image_id\"] = data_tr[\"image_id\"].astype(np.int32)\ndata_tr.info()","execution_count":15,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10661 entries, 0 to 10660\nData columns (total 7 columns):\nPatient_ID    10661 non-null object\npath          10661 non-null object\nimage         10661 non-null object\nsubject_id    10661 non-null int32\nimage_id      10661 non-null int32\ncell_no       10661 non-null int32\nlabels        10661 non-null bool\ndtypes: bool(1), int32(3), object(3)\nmemory usage: 385.3+ KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_tr.drop([\"labels\"], axis = 1)\ny = data_tr[\"labels\"]\nX.head(4)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"          Patient_ID   ...   cell_no\n0   UID_H14_26_1_hem   ...         1\n1   UID_H10_63_1_hem   ...         1\n2   UID_H14_30_6_hem   ...         6\n3  UID_H10_190_3_hem   ...         3\n\n[4 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient_ID</th>\n      <th>path</th>\n      <th>image</th>\n      <th>subject_id</th>\n      <th>image_id</th>\n      <th>cell_no</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UID_H14_26_1_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>14</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UID_H10_63_1_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>10</td>\n      <td>63</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UID_H14_30_6_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>14</td>\n      <td>30</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>UID_H10_190_3_hem</td>\n      <td>../input/blood-cancer-training-dataset/c-nmc_t...</td>\n      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n      <td>10</td>\n      <td>190</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\ny_train.value_counts().plot(kind = \"bar\")\n\nprint(\"%age of label value 0 in training dataset\", \n      y_train.value_counts()[0]/(y_train.value_counts()[0]+y_train.value_counts()[1]))\nprint(\"%age of label value 0 in test dataset\", \n      y_test.value_counts()[0]/(y_test.value_counts()[0]+y_test.value_counts()[1]))\n\ndel X, y","execution_count":17,"outputs":[{"output_type":"stream","text":"%age of label value 0 in training dataset 0.3170731707317073\n%age of label value 0 in test dataset 0.32114392873886544\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts().plot(kind = \"bar\")","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f2d2f2d9e80>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization\nx_train = np.asarray(x_train['image'].tolist())\nx_test = np.asarray(x_test['image'].tolist())\n\nx_train_mean = np.mean(x_train)\nx_test_mean = np.mean(x_test)\n\nx_train_std = np.std(x_train)\nx_test_std = np.std(x_test)\n\nx_train = (x_train - x_train_mean)/x_train_std\nx_test = (x_test - x_test_mean)/x_test_std\n\ny_dopetest = y_test\n# Label Encoding\ny_train = to_categorical(y_train, num_classes = 2)\ny_test = to_categorical(y_test, num_classes = 2)\n\n# Reshape images in 3 dimensions\nx_train = x_train.reshape(x_train.shape[0], *(100, 100, 3))\nx_test = x_test.reshape(x_test.shape[0], *(100, 100, 3))\nx_train.shape","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"(8528, 100, 100, 3)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Defining models to train on our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (100, 100, 3)\nnum_classes = 2\n# Set learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc', patience = 3, verbose = 1, \n                                           factor = 0.5, min_lr = 0.00001)\n\n# Data Augmentation\ndatagen = ImageDataGenerator(featurewise_center = False, samplewise_center = True,\n                            featurewise_std_normalization = False, samplewise_std_normalization = False, \n                            zca_whitening = False, rotation_range = 0, zoom_range = 0.3, \n                            width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = False, \n                            vertical_flip = False, rescale = 1./255)\ndatagen.fit(x_train)\n# print(datagen.mean, datagen.std)\n# datagen.standardize(x_test)\nprint(x_test.shape)\nfor i in range(len(x_test)):\n    # this is what you are looking for\n    x_test[i] = datagen.standardize(x_test[i])\nprint(x_test.shape)\n\n# Define the optimizer\n# optimizer = Adam(lr = .001, beta_1 = .9, beta_2 = .999, epsilon = None, decay = .0, amsgrad = False)\n# Fit the model\n# \n# epochs = 20\n# batch_size = 80","execution_count":41,"outputs":[{"output_type":"stream","text":"(2133, 100, 100, 3)\n(2133, 100, 100, 3)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Defining and fitting Capsule network"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom __future__ import print_function\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras import activations\nfrom keras import utils\n# from keras.datasets import cifar10\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n# the squashing function.\n# we use 0.5 instead of 1 in hinton's paper.\n# if 1, the norm of vector will be zoomed out.\n# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n# and be zoomed out while original norm is greater than 0.5.\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n    return scale * x\n\n\n# define our own softmax function instead of K.softmax\n# because K.softmax can not specify axis.\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex / K.sum(ex, axis=axis, keepdims=True)\n\n\n# define the margin loss like hinge loss\ndef margin_loss(y_true, y_pred):\n    lamb, margin = 0.5, 0.1\n    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n\n\nclass Capsule(Layer):\n    \"\"\"A Capsule Implement with Pure Keras\n    There are two vesions of Capsule.\n    One is like dense layer (for the fixed-shape input),\n    and the other is like time distributed dense (for various length input).\n\n    The input shape of Capsule must be (batch_size,\n                                        input_num_capsule,\n                                        input_dim_capsule\n                                       )\n    and the output shape is (batch_size,\n                             num_capsule,\n                             dim_capsule\n                            )\n\n    Capsule Implement is from https://github.com/bojone/Capsule/\n    Capsule Paper: https://arxiv.org/abs/1710.09829\n    \"\"\"\n\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 activation='squash',\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(1, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(input_num_capsule, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n\n    def call(self, inputs):\n        \"\"\"Following the routing algorithm from Hinton's paper,\n        but replace b = b + <u,v> with b = <u,v>.\n\n        This change can improve the feature representation of Capsule.\n\n        However, you can replace\n            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n        with\n            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n        to realize a standard routing.\n        \"\"\"\n\n        if self.share_weights:\n            hat_inputs = K.conv1d(inputs, self.kernel)\n        else:\n            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        hat_inputs = K.reshape(hat_inputs,\n                               (batch_size, input_num_capsule,\n                                self.num_capsule, self.dim_capsule))\n        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n\n        b = K.zeros_like(hat_inputs[:, :, :, 0])\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(o, hat_inputs, [2, 3])\n                if K.backend() == 'keras':\n                    o = K.sum(o, axis=1)\n\n        return o\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n\nbatch_size = 128\nnum_classes = 2\nepochs = 10\n# # (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# # x_train = x_train.astype('float32')\n# # x_test = x_test.astype('float32')\n# # x_train /= 255\n# # x_test /= 255\n# # y_train = utils.to_categorical(y_train, num_classes)\n# # y_test = utils.to_categorical(y_test, num_classes)\n\n# A common Conv2D model\ninput_image = Input(shape=(None, None, 3))\nx = Conv2D(64, (3, 3), activation='relu')(input_image)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = AveragePooling2D((2, 2))(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\nx = AveragePooling2D((2, 2))(x)\n\n\n\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\nthen connect a Capsule layer.\n\nthe output of final model is the lengths of 10 Capsule, whose dim=16.\n\nthe length of Capsule is the proba,\nso the problem becomes a 10 two-classification problem.\n\"\"\"\n\nx = Reshape((-1, 128))(x)\ncapsule = Capsule(2, 16, 3, True)(x)\ncapsule = Dense(1, activation = 'sigmoid')(capsule)\noutput = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\nmodel_caps = Model(inputs=input_image, outputs=output)\n\n# we use a margin loss\nmodel_caps.compile(loss=margin_loss, optimizer= optimizers.Adam(lr = 0.001), metrics=['mae', 'acc'])\nmodel_caps.summary()\n\n# we can compare the performance with or without data augmentation\ndata_augmentation = False\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model_caps.fit_generator(\n        datagen.flow(x_train, y_train, batch_size=batch_size),\n        epochs=epochs,\n        validation_data=(x_test, y_test),\n        steps_per_epoch = x_train.shape[0] // batch_size)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen1 = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by dataset std\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically\n        shear_range=0.,  # set range for random shear\n        zoom_range=0.,  # set range for random zoom\n        channel_shift_range=0.,  # set range for random channel shifts\n        # set mode for filling points outside the input boundaries\n        fill_mode='nearest',\n        cval=0.,  # value used for fill_mode = \"constant\"\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False,  # randomly flip images\n        # set rescaling factor (applied before any other transformation)\n        rescale=None,\n        # set function that will be applied on each input\n        preprocessing_function=None,\n        # image data format, either \"channels_first\" or \"channels_last\"\n        data_format=None,\n        # fraction of images reserved for validation (strictly between 0 and 1)\n        validation_split=0.0)\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(\n        datagen1.flow(x_train, y_train, batch_size=batch_size),\n        epochs=epochs,\n        validation_data=(x_test, y_test),\n        steps_per_epoch = x_train.shape[0] // batch_size,\n        workers=4)\n\n","execution_count":42,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, None, None, 3)     0         \n_________________________________________________________________\nconv2d_550 (Conv2D)          (None, None, None, 64)    1792      \n_________________________________________________________________\nconv2d_551 (Conv2D)          (None, None, None, 64)    36928     \n_________________________________________________________________\naverage_pooling2d_1 (Average (None, None, None, 64)    0         \n_________________________________________________________________\nconv2d_552 (Conv2D)          (None, None, None, 128)   73856     \n_________________________________________________________________\nconv2d_553 (Conv2D)          (None, None, None, 128)   147584    \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, None, None, 128)   0         \n_________________________________________________________________\nreshape_1 (Reshape)          (None, None, 128)         0         \n_________________________________________________________________\ncapsule_1 (Capsule)          (None, 2, 16)             4096      \n_________________________________________________________________\ndense_3 (Dense)              (None, 2, 1)              17        \n_________________________________________________________________\nlambda_513 (Lambda)          (None, 2)                 0         \n=================================================================\nTotal params: 264,273\nTrainable params: 264,273\nNon-trainable params: 0\n_________________________________________________________________\nNot using data augmentation.\nEpoch 1/10\n66/66 [==============================] - 28s 429ms/step - loss: 0.1882 - mean_absolute_error: 0.4508 - acc: 0.6828 - val_loss: 0.1875 - val_mean_absolute_error: 0.4529 - val_acc: 0.6789\nEpoch 2/10\n66/66 [==============================] - 18s 269ms/step - loss: 0.1626 - mean_absolute_error: 0.4103 - acc: 0.7327 - val_loss: 0.1584 - val_mean_absolute_error: 0.4025 - val_acc: 0.7089\nEpoch 3/10\n66/66 [==============================] - 19s 282ms/step - loss: 0.1546 - mean_absolute_error: 0.3954 - acc: 0.7490 - val_loss: 0.1551 - val_mean_absolute_error: 0.4038 - val_acc: 0.7079\nEpoch 4/10\n66/66 [==============================] - 18s 279ms/step - loss: 0.1529 - mean_absolute_error: 0.3888 - acc: 0.7514 - val_loss: 0.1577 - val_mean_absolute_error: 0.4065 - val_acc: 0.6957\nEpoch 5/10\n66/66 [==============================] - 18s 279ms/step - loss: 0.1556 - mean_absolute_error: 0.3947 - acc: 0.7424 - val_loss: 0.1507 - val_mean_absolute_error: 0.4064 - val_acc: 0.7482\nEpoch 6/10\n66/66 [==============================] - 18s 280ms/step - loss: 0.1529 - mean_absolute_error: 0.3881 - acc: 0.7509 - val_loss: 0.1692 - val_mean_absolute_error: 0.4034 - val_acc: 0.6803\nEpoch 7/10\n66/66 [==============================] - 19s 282ms/step - loss: 0.1518 - mean_absolute_error: 0.3873 - acc: 0.7509 - val_loss: 0.1503 - val_mean_absolute_error: 0.4103 - val_acc: 0.7689\nEpoch 8/10\n66/66 [==============================] - 19s 281ms/step - loss: 0.1548 - mean_absolute_error: 0.3912 - acc: 0.7451 - val_loss: 0.1568 - val_mean_absolute_error: 0.4112 - val_acc: 0.7042\nEpoch 9/10\n66/66 [==============================] - 18s 279ms/step - loss: 0.1539 - mean_absolute_error: 0.3912 - acc: 0.7488 - val_loss: 0.1784 - val_mean_absolute_error: 0.3949 - val_acc: 0.6793\nEpoch 10/10\n66/66 [==============================] - 19s 281ms/step - loss: 0.1553 - mean_absolute_error: 0.3931 - acc: 0.7437 - val_loss: 0.1613 - val_mean_absolute_error: 0.4049 - val_acc: 0.6854\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Fitting our pretrained models"},{"metadata":{},"cell_type":"markdown","source":"Densenet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.densenet import DenseNet121\n\nbatch_size = 128\nnum_classes = 2\nepochs = 10\n\nmodel_densenet = Sequential()\nmodel_densenet.add(DenseNet121(include_top=False, pooling= 'max', weights = 'imagenet'))\nmodel_densenet.add(BatchNormalization())\nmodel_densenet.add(Activation(\"tanh\"))\nmodel_densenet.add(Dropout(0.40))\nmodel_densenet.add(Dense(num_classes, activation = \"sigmoid\"))\n\n# model_densenet.layers[0].trainable = False\n# model_densenet.layers[1].trainable = False\n\nmodel_densenet.compile(optimizer = \"Adamax\", loss = \"categorical_crossentropy\", \n                       metrics = ['mae', 'accuracy'])\n\ndatagen = ImageDataGenerator(featurewise_center=True,  # set input mean to 0 over the dataset\n                             samplewise_center=False,  # set each sample mean to 0\n                             featurewise_std_normalization=True,  # divide inputs by dataset std\n                             samplewise_std_normalization=False,  # divide each input by its std\n                             zca_whitening=False,  # apply ZCA whitening\n                             zca_epsilon=1e-06,  # epsilon for ZCA whitening\n                             rotation_range=0.2,  # randomly rotate images in 0 to 180 degrees\n                             width_shift_range=0.1,  # randomly shift images horizontally\n                             height_shift_range=0.1,  # randomly shift images vertically\n                             shear_range=0.,  # set range for random shear\n                             zoom_range=0.3,  # set range for random zoom\n                             channel_shift_range=0,  # set range for random channel shifts\n                             # set mode for filling points outside the input boundaries\n                             fill_mode='nearest',\n                             cval=0.,  # value used for fill_mode = \"constant\"\n                             horizontal_flip=True,  # randomly flip images\n                             vertical_flip=False,  # randomly flip images\n                             # set rescaling factor (applied before any other transformation)\n                             rescale=1./255,\n                             # set function that will be applied on each input\n                             preprocessing_function=None,\n                             # image data format, either \"channels_first\" or \"channels_last\"\n                             data_format=None,\n                             # fraction of images reserved for validation (strictly between 0 and 1)\n                             validation_split=0.0)\ndatagen.fit(x_train)\n# print(datagen.mean, datagen.std)\n# datagen.standardize(x_test)\nprint(x_test.shape)\nfor i in range(len(x_test)):\n    # this is what you are looking for\n    x_test[i] = datagen.standardize(x_test[i])\nprint(x_test.shape)\n\n\nprint(\"*\"*40 + \"Densenet model\" + \"*\"*40)\nhistory = model_densenet.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                           verbose = 1,\n                           epochs = 3,\n                           steps_per_epoch= x_train.shape[0] // batch_size, \n                            callbacks = [learning_rate_reduction],\n                           validation_data = (x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_densenet.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining ResNeXt50 model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sys.path.append('/kaggle/working/isbi2019cancer/')\n# import isbi2019cancer.utils\n# sys.modules[\"utils\"] = isbi2019cancer.utils\n# import isbi2019cancer.model as model_jprell","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''ResNeXt models for Keras.\n# Reference\n- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf))\n'''\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.core import Activation\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import GlobalAveragePooling2D, GlobalMaxPooling2D, MaxPooling2D\nfrom keras.layers import Input\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\n# from keras.applications.imagenet_utils import _obtain_input_shape\nimport keras.backend as K\n\nCIFAR_TH_WEIGHTS_PATH = ''\nCIFAR_TF_WEIGHTS_PATH = ''\nCIFAR_TH_WEIGHTS_PATH_NO_TOP = ''\nCIFAR_TF_WEIGHTS_PATH_NO_TOP = ''\n\nIMAGENET_TH_WEIGHTS_PATH = ''\nIMAGENET_TF_WEIGHTS_PATH = ''\nIMAGENET_TH_WEIGHTS_PATH_NO_TOP = ''\nIMAGENET_TF_WEIGHTS_PATH_NO_TOP = ''","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def __create_res_next_imagenet(nb_classes, img_input, include_top, depth, cardinality=32, width=4,\n                               weight_decay=5e-4, pooling=None):\n    ''' Creates a ResNeXt model with specified parameters\n    Args:\n        nb_classes: Number of output classes\n        img_input: Input tensor or layer\n        include_top: Flag to include the last dense layer\n        depth: Depth of the network. List of integers.\n               Increasing cardinality improves classification accuracy,\n        width: Width of the network.\n        weight_decay: weight_decay (l2 norm)\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n    Returns: a Keras Model\n    '''\n\n    if type(depth) is list or type(depth) is tuple:\n        # If a list is provided, defer to user how many blocks are present\n        N = list(depth)\n    else:\n        # Otherwise, default to 3 blocks each of default number of group convolution blocks\n        N = [(depth - 2) // 9 for _ in range(3)]\n\n    filters = cardinality * width\n    filters_list = []\n\n    for i in range(len(N)):\n        filters_list.append(filters)\n        filters *= 2  # double the size of the filters\n\n    x = __initial_conv_block_imagenet(img_input, weight_decay)\n\n    # block 1 (no pooling)\n    for i in range(N[0]):\n        x = __bottleneck_block(x, filters_list[0], cardinality, strides=1, weight_decay=weight_decay)\n\n    N = N[1:]  # remove the first block from block definition list\n    filters_list = filters_list[1:]  # remove the first filter from the filter list\n\n    # block 2 to N\n    for block_idx, n_i in enumerate(N):\n        for i in range(n_i):\n            if i == 0:\n                x = __bottleneck_block(x, filters_list[block_idx], cardinality, strides=2,\n                                       weight_decay=weight_decay)\n            else:\n                x = __bottleneck_block(x, filters_list[block_idx], cardinality, strides=1,\n                                       weight_decay=weight_decay)\n\n    if include_top:\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(nb_classes, use_bias=False, kernel_regularizer=l2(weight_decay),\n                  kernel_initializer='he_normal', activation='softmax')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n    return x\n\ndef __initial_conv_block_imagenet(input, weight_decay=5e-4):\n    ''' Adds an initial conv block, with batch norm and relu for the inception resnext\n    Args:\n        input: input tensor\n        weight_decay: weight decay factor\n    Returns: a keras tensor\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = Conv2D(64, (7, 7), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay), strides=(2, 2))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n\n    return x\n\ndef __bottleneck_block(input, filters=64, cardinality=8, strides=1, weight_decay=5e-4):\n    ''' Adds a bottleneck block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        cardinality: cardinality factor described number of\n            grouped convolutions\n        strides: performs strided convolution for downsampling if > 1\n        weight_decay: weight decay factor\n    Returns: a keras tensor\n    '''\n    init = input\n\n    grouped_channels = int(filters / cardinality)\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    # Check if input number of filters is same as 16 * k, else create convolution2d for this input\n    if K.image_data_format() == 'channels_first':\n        if init._keras_shape[1] != 2 * filters:\n            init = Conv2D(filters * 2, (1, 1), padding='same', strides=(strides, strides),\n                          use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n            init = BatchNormalization(axis=channel_axis)(init)\n    else:\n        if init._keras_shape[-1] != 2 * filters:\n            init = Conv2D(filters * 2, (1, 1), padding='same', strides=(strides, strides),\n                          use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n            init = BatchNormalization(axis=channel_axis)(init)\n\n    x = Conv2D(filters, (1, 1), padding='same', use_bias=False,\n               kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n\n    x = __grouped_convolution_block(x, grouped_channels, cardinality, strides, weight_decay)\n\n    x = Conv2D(filters * 2, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay))(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n\n    x = add([init, x])\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef __grouped_convolution_block(input, grouped_channels, cardinality, strides, weight_decay=5e-4):\n    ''' Adds a grouped convolution block. It is an equivalent block from the paper\n    Args:\n        input: input tensor\n        grouped_channels: grouped number of filters\n        cardinality: cardinality factor describing the number of groups\n        strides: performs strided convolution for downscaling if > 1\n        weight_decay: weight decay term\n    Returns: a keras tensor\n    '''\n    init = input\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    group_list = []\n\n    if cardinality == 1:\n        # with cardinality 1, it is a standard convolution\n        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=(strides, strides),\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n        x = BatchNormalization(axis=channel_axis)(x)\n        x = Activation('relu')(x)\n        return x\n\n    for c in range(cardinality):\n        x = Lambda(lambda z: z[:, :, :, c * grouped_channels:(c + 1) * grouped_channels]\n        if K.image_data_format() == 'channels_last' else\n        lambda z: z[:, c * grouped_channels:(c + 1) * grouped_channels, :, :])(input)\n\n        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=(strides, strides),\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n\n        group_list.append(x)\n\n    group_merge = concatenate(group_list, axis=channel_axis)\n    x = BatchNormalization(axis=channel_axis)(group_merge)\n    x = Activation('relu')(x)\n\n    return x","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (100, 100, 3)\nnum_classes = 2\nbatch_size = 128\n# Data Augmentation\ndatagen = ImageDataGenerator(featurewise_center = False, samplewise_center = True,\n                            featurewise_std_normalization = False, samplewise_std_normalization = True, \n                            zca_whitening = False, rotation_range = 0.3, zoom_range = 0.3, \n                            width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = True, \n                            vertical_flip = False, rescale = 1./255)\ndatagen.fit(x_train)\n# print(datagen.mean, datagen.std)\n# datagen.standardize(x_test)\nprint(x_test.shape)\nfor i in range(len(x_test)):\n    # this is what you are looking for\n    x_test[i] = datagen.standardize(x_test[i])\nprint(x_test.shape)","execution_count":23,"outputs":[{"output_type":"stream","text":"(2133, 100, 100, 3)\n(2133, 100, 100, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ResNextImageNet(input_shape, depth, cardinality, width, weight_decay, pooling, classes, \n                    include_top= False, weights=None, input_tensor=None):\n    \"\"\" Instantiate the ResNeXt architecture for the ImageNet dataset. Note that ,\n        when using TensorFlow for best performance you should set\n        `image_data_format=\"channels_last\"` in your Keras config\n        at ~/.keras/keras.json.\n        The model are compatible with both\n        TensorFlow and Theano. The dimension ordering\n        convention used by the model is the one\n        specified in your Keras config file.\n        # Arguments\n            depth: number or layers in the each block, defined as a list.\n                ResNeXt-50 can be defined as [3, 4, 6, 3].\n                ResNeXt-101 can be defined as [3, 4, 23, 3].\n                Defaults is ResNeXt-50.\n            cardinality: the size of the set of transformations\n            width: multiplier to the ResNeXt width (number of filters)\n            weight_decay: weight decay (l2 norm)\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: `None` (random initialization) or `imagenet` (trained\n                on ImageNet)\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(224, 224, 3)` (with `tf` dim ordering)\n                or `(3, 224, 224)` (with `th` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            pooling: Optional pooling mode for feature extraction\n                when `include_top` is `False`.\n                - `None` means that the output of the model will be\n                    the 4D tensor output of the\n                    last convolutional layer.\n                - `avg` means that global average pooling\n                    will be applied to the output of the\n                    last convolutional layer, and thus\n                    the output of the model will be a 2D tensor.\n                - `max` means that global max pooling will\n                    be applied.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n        # Returns\n            A Keras model instance.\n        \"\"\"\n\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n\n    if weights == 'imagenet' and include_top and classes != 1000:\n        raise ValueError('If using `weights` as imagenet with `include_top`'\n                         ' as true, `classes` should be 1000')\n\n    if type(depth) == int and (depth - 2) % 9 != 0:\n        raise ValueError('Depth of the network must be such that (depth - 2)'\n                         'should be divisible by 9.')\n    # Determine proper input shape\n#     input_shape = _obtain_input_shape(input_shape,\n#                                       default_size=224,\n#                                       min_size=112,\n#                                       data_format=K.image_data_format(),\n#                                       require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_res_next_imagenet(classes, img_input, include_top, depth, cardinality, width,\n                                   weight_decay, pooling)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='resnext')\n\n    # load weights\n    if weights == 'imagenet':\n        if (depth == [3, 4, 6, 3]) and (cardinality == 32) and (width == 4):\n            # Default parameters match. Weights for this model exist:\n\n            if K.image_data_format() == 'channels_first':\n                if include_top:\n                    weights_path = get_file('resnext_imagenet_32_4_th_dim_ordering_th_kernels.h5',\n                                            IMAGENET_TH_WEIGHTS_PATH,\n                                            cache_subdir='models')\n                else:\n                    weights_path = get_file('resnext_imagenet_32_4_th_dim_ordering_th_kernels_no_top.h5',\n                                            IMAGENET_TH_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir='models')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == 'tensorflow':\n                    warnings.warn('You are using the TensorFlow backend, yet you '\n                                  'are using the Theano '\n                                  'image dimension ordering convention '\n                                  '(`image_dim_ordering=\"th\"`). '\n                                  'For best performance, set '\n                                  '`image_dim_ordering=\"tf\"` in '\n                                  'your Keras config '\n                                  'at ~/.keras/keras.json.')\n                    convert_all_kernels_in_model(model)\n            else:\n                if include_top:\n                    weights_path = get_file('resnext_imagenet_32_4_tf_dim_ordering_tf_kernels.h5',\n                                            IMAGENET_TF_WEIGHTS_PATH,\n                                            cache_subdir='models')\n                else:\n                    weights_path = get_file('resnext_imagenet_32_4_tf_dim_ordering_tf_kernels_no_top.h5',\n                                            IMAGENET_TF_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir='models')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == 'theano':\n                    convert_all_kernels_in_model(model)\n\n    return model","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnext = Sequential()\nmodel_resnext.add(ResNextImageNet(input_shape= input_shape, depth=[3, 4, 6, 3], cardinality=32, width=4, \n                                weight_decay=5e-4, include_top= False, weights=None, input_tensor=None, \n                                pooling= 'max', classes= 1000))\nmodel_resnext.add(Dropout(.3))\nmodel_resnext.add(Dense(num_classes, activation = \"sigmoid\"))","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnext.summary()","execution_count":26,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnext (Model)              (None, 2048)              23048128  \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 4098      \n=================================================================\nTotal params: 23,052,226\nTrainable params: 22,984,002\nNon-trainable params: 68,224\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnext.compile(optimizer = \"Adamax\", loss = 'categorical_crossentropy', metrics = ['mae', 'accuracy'])","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnext.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size), verbose = 1, \n                           epochs = 20,\n                           steps_per_epoch= x_train.shape[0] // batch_size, \n                            callbacks = [learning_rate_reduction],\n                           validation_data = (x_test, y_test))","execution_count":28,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n66/66 [==============================] - 96s 1s/step - loss: 31.9962 - mean_absolute_error: 0.3180 - acc: 0.6817 - val_loss: 29.1369 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 2/20\n66/66 [==============================] - 32s 488ms/step - loss: 27.8693 - mean_absolute_error: 0.3167 - acc: 0.6833 - val_loss: 26.8733 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 3/20\n66/66 [==============================] - 32s 481ms/step - loss: 25.9720 - mean_absolute_error: 0.3183 - acc: 0.6817 - val_loss: 25.1952 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 4/20\n66/66 [==============================] - 32s 483ms/step - loss: 24.3968 - mean_absolute_error: 0.3156 - acc: 0.6844 - val_loss: 23.7912 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\nEpoch 5/20\n66/66 [==============================] - 32s 478ms/step - loss: 23.4430 - mean_absolute_error: 0.3192 - acc: 0.6808 - val_loss: 23.1533 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 6/20\n66/66 [==============================] - 32s 478ms/step - loss: 22.7952 - mean_absolute_error: 0.3179 - acc: 0.6821 - val_loss: 22.5345 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 7/20\n66/66 [==============================] - 31s 477ms/step - loss: 22.1183 - mean_absolute_error: 0.3138 - acc: 0.6862 - val_loss: 21.9325 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\nEpoch 8/20\n66/66 [==============================] - 31s 477ms/step - loss: 21.7119 - mean_absolute_error: 0.3166 - acc: 0.6834 - val_loss: 21.6333 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 9/20\n66/66 [==============================] - 31s 476ms/step - loss: 21.4615 - mean_absolute_error: 0.3198 - acc: 0.6802 - val_loss: 21.3285 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 10/20\n66/66 [==============================] - 31s 474ms/step - loss: 21.0636 - mean_absolute_error: 0.3142 - acc: 0.6858 - val_loss: 21.0181 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\nEpoch 11/20\n66/66 [==============================] - 31s 477ms/step - loss: 20.9232 - mean_absolute_error: 0.3201 - acc: 0.6799 - val_loss: 20.8586 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 12/20\n66/66 [==============================] - 31s 476ms/step - loss: 20.6698 - mean_absolute_error: 0.3145 - acc: 0.6855 - val_loss: 20.6926 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 13/20\n66/66 [==============================] - 32s 478ms/step - loss: 20.5535 - mean_absolute_error: 0.3178 - acc: 0.6822 - val_loss: 20.5199 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n\nEpoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\nEpoch 14/20\n66/66 [==============================] - 31s 477ms/step - loss: 20.4137 - mean_absolute_error: 0.3173 - acc: 0.6827 - val_loss: 20.4297 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 15/20\n66/66 [==============================] - 31s 475ms/step - loss: 20.3390 - mean_absolute_error: 0.3184 - acc: 0.6816 - val_loss: 20.3347 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 16/20\n66/66 [==============================] - 31s 476ms/step - loss: 20.1710 - mean_absolute_error: 0.3140 - acc: 0.6860 - val_loss: 20.2349 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\nEpoch 17/20\n66/66 [==============================] - 31s 476ms/step - loss: 20.1976 - mean_absolute_error: 0.3204 - acc: 0.6796 - val_loss: 20.1822 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 18/20\n66/66 [==============================] - 31s 474ms/step - loss: 20.0569 - mean_absolute_error: 0.3151 - acc: 0.6849 - val_loss: 20.1264 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\nEpoch 19/20\n66/66 [==============================] - 31s 475ms/step - loss: 20.0890 - mean_absolute_error: 0.3206 - acc: 0.6794 - val_loss: 20.0674 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\nEpoch 20/20\n66/66 [==============================] - 31s 473ms/step - loss: 19.9274 - mean_absolute_error: 0.3134 - acc: 0.6866 - val_loss: 20.0361 - val_mean_absolute_error: 0.3211 - val_acc: 0.6789\n","name":"stdout"},{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"<keras.callbacks.History at 0x7f2d28183518>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnext.evaluate(x_test, y_test, verbose = 1)","execution_count":29,"outputs":[{"output_type":"stream","text":"2133/2133 [==============================] - 4s 2ms/step\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[20.036126340212423, 0.32114392874585146, 0.6788560714008548]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_se_net = Sequential()\n# model_se_net.add(model_senet)\n# model_se_net.add(Dense(num_classes, activation = 'sigmoid'))\n# model_senet.compile(optimizer = \"Adamax\", loss = 'categorical_crossentropy', metrics = ['mae', 'accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/titu1994/Keras-ResNeXt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\nfrom keras.applications import ResNet50, VGG16, VGG19, InceptionV3\n# Using pretrained models for VGG16, VGG19, resnet, InceptionV3\n# If you are using Kaggle kernel, do switch ON the Internet from Settings.\n# from keras.applications.resnext import ResNeXt50\n# from keras.applications.nasnet import NASNetMobile\n# model_nasnet = Sequential()\n# model_nasnet.add(NASNetMobile(include_top = None, input_shape = (100, 100, 3), pooling='max'))\n# model_nasnet.add(Dense(num_classes, activation = \"sigmoid\"))\n\n# from keras.applications.mobilenet_v2 import MobileNetV2\n# model_mobile = Sequential()\n# model_mobile.add(MobileNetV2(input_shape=None, alpha=1.0, \n#                         include_top=False, weights='imagenet', input_tensor=None, pooling='max'))\n# model_mobile.add(Dense(num_classes, activation = \"softmax\"))\n\n# model_resnet = Sequential()\n# model_resnet.add(ResNet50(include_top=False, pooling='max', weights= \"imagenet\"))\n# model_resnet.add(Dense(num_classes, activation='softmax'))\n\n# model_vgg16 = Sequential()\n# model_vgg16.add(VGG16(include_top= False, pooling = \"max\", weights = \"imagenet\"))\n# model_vgg16.add(Dense(num_classes, activation = \"softmax\"))\n\nmodel_vgg19 = Sequential()\nmodel_vgg19.add(VGG19(include_top= False, pooling = \"max\", weights = \"imagenet\"))\nmodel_vgg19.add(Dense(num_classes, activation = \"softmax\"))\n\n# model_inception = Sequential()\n# model_inception.add(InceptionV3(include_top= False, pooling = \"max\", weights = \"imagenet\"))\n# model_inception.add(Dense(num_classes, activation = \"softmax\"))\n\n# Say not to train first layer (ResNet) model. It is already trained\n# model_nasnet.layers[0].trainable = False\n# model_mobile.layers[0].trainable = False\n# model_resnet.layers[0].trainable = False\n# model_vgg16.layers[0].trainable = False\nmodel_vgg19.layers[0].trainable = False\n# model_inception.layers[0].trainable = False","execution_count":30,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80142336/80134624 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the pretrained models\n# model_nasnet.compile(optimizer= \"Adamax\", loss='categorical_crossentropy', metrics=['accuracy'])\n\n# model_mobile.compile(optimizer= \"Adamax\", loss='categorical_crossentropy', metrics=['accuracy'])\n\n# model_resnet.compile(optimizer= \"Adamax\", loss='categorical_crossentropy', metrics=['accuracy'])\n\n# model_vgg16.compile(optimizer = \"Adamax\", loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n\nmodel_vgg19.compile(optimizer = \"Adamax\", loss = \"categorical_crossentropy\", metrics = ['mae', 'accuracy'])\n\n# model_inception.compile(optimizer = 'Adamax', loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n\ndatagen = ImageDataGenerator(featurewise_center = False, samplewise_center = True,\n                            featurewise_std_normalization = False, samplewise_std_normalization = True, \n                            zca_whitening = False, rotation_range = 0, zoom_range = 0.3, \n                            width_shift_range = 0.2, height_shift_range = 0.2, horizontal_flip = False, \n                            vertical_flip = False, rescale = 1./255)\ndatagen.fit(x_train)\n# print(datagen.mean, datagen.std)\n# datagen.standardize(x_test)\nprint(x_test.shape)\nfor i in range(len(x_test)):\n    # this is what you are looking for\n    x_test[i] = datagen.standardize(x_test[i])\nprint(x_test.shape)","execution_count":31,"outputs":[{"output_type":"stream","text":"(2133, 100, 100, 3)\n(2133, 100, 100, 3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom.seed(1234)\n# reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2,\n#                               patience=3, min_lr=0.001)\n# Fit the model\n# print(\"*\"*40 + \"Nasnet model\" + \"*\"*40)\n# history = model_nasnet.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                            verbose = 0,\n#                            epochs = epochs,\n#                            steps_per_epoch=x_train.shape[0] // batch_size,\n#                                      callbacks = [learning_rate_reduction],\n#                            validation_data = (x_test, y_test))\n\n# print(\"*\"*40 + \"Mobile model\" + \"*\"*40)\n# history = model_mobile.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                            verbose = 0,\n#                            epochs = epochs,\n#                            steps_per_epoch=x_train.shape[0] // batch_size,\n#                                      callbacks = [learning_rate_reduction],\n#                            validation_data = (x_test, y_test))\n\n# print(\"*\"*40 + \"ResNet50 model\" + \"*\"*40)\n# history = model_resnet.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                            verbose = 1,\n#                            epochs = epochs,\n#                            steps_per_epoch=x_train.shape[0] // batch_size,\n#                                      callbacks = [learning_rate_reduction],\n#                            validation_data = (x_test, y_test))\n# print(\"*\"*40 + \"VGG16 model\" + \"*\"*40)\n# history = model_vgg16.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                            verbose = 1,\n#                            epochs = epochs,\n#                            steps_per_epoch=x_train.shape[0] // batch_size, \n#                             callbacks = [learning_rate_reduction], \n#                             validation_data = (x_test, y_test))\nbatch_size = 128\nnum_classes = 2\nepochs = 10\nprint(\"*\"*40 + \"VGG19 model\" + \"*\"*40)\nhistory = model_vgg19.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                           verbose = 1,\n                           epochs = epochs,\n                           steps_per_epoch= x_train.shape[0] // batch_size, \n                            callbacks = [learning_rate_reduction],\n                           validation_data = (x_test, y_test))\n\n# print(\"*\"*40 + \"inception model\" + \"*\"*40)\n# history = model_inception.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                            verbose = 1,\n#                            epochs = epochs,\n#                            steps_per_epoch=x_train.shape[0] // batch_size,\n#                             callbacks = [learning_rate_reduction], \n#                            validation_data = (x_test, y_test))","execution_count":32,"outputs":[{"output_type":"stream","text":"****************************************VGG19 model****************************************\nEpoch 1/10\n66/66 [==============================] - 28s 427ms/step - loss: 0.5726 - mean_absolute_error: 0.3920 - acc: 0.7214 - val_loss: 0.5025 - val_mean_absolute_error: 0.3605 - val_acc: 0.7867\nEpoch 2/10\n66/66 [==============================] - 18s 266ms/step - loss: 0.5282 - mean_absolute_error: 0.3593 - acc: 0.7509 - val_loss: 0.4791 - val_mean_absolute_error: 0.3334 - val_acc: 0.7881\nEpoch 3/10\n66/66 [==============================] - 19s 290ms/step - loss: 0.5107 - mean_absolute_error: 0.3446 - acc: 0.7675 - val_loss: 0.4707 - val_mean_absolute_error: 0.3183 - val_acc: 0.7886\nEpoch 4/10\n66/66 [==============================] - 19s 289ms/step - loss: 0.5104 - mean_absolute_error: 0.3418 - acc: 0.7668 - val_loss: 0.4686 - val_mean_absolute_error: 0.3135 - val_acc: 0.7867\nEpoch 5/10\n66/66 [==============================] - 20s 299ms/step - loss: 0.5042 - mean_absolute_error: 0.3374 - acc: 0.7656 - val_loss: 0.4634 - val_mean_absolute_error: 0.3106 - val_acc: 0.7928\nEpoch 6/10\n66/66 [==============================] - 19s 292ms/step - loss: 0.5072 - mean_absolute_error: 0.3348 - acc: 0.7695 - val_loss: 0.4676 - val_mean_absolute_error: 0.3017 - val_acc: 0.7895\nEpoch 7/10\n66/66 [==============================] - 19s 291ms/step - loss: 0.4977 - mean_absolute_error: 0.3285 - acc: 0.7761 - val_loss: 0.4660 - val_mean_absolute_error: 0.3009 - val_acc: 0.7909\nEpoch 8/10\n66/66 [==============================] - 19s 289ms/step - loss: 0.4969 - mean_absolute_error: 0.3285 - acc: 0.7740 - val_loss: 0.4656 - val_mean_absolute_error: 0.3020 - val_acc: 0.7900\n\nEpoch 00008: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\nEpoch 9/10\n66/66 [==============================] - 17s 265ms/step - loss: 0.4979 - mean_absolute_error: 0.3298 - acc: 0.7731 - val_loss: 0.4675 - val_mean_absolute_error: 0.3013 - val_acc: 0.7881\nEpoch 10/10\n66/66 [==============================] - 19s 289ms/step - loss: 0.5048 - mean_absolute_error: 0.3331 - acc: 0.7695 - val_loss: 0.4654 - val_mean_absolute_error: 0.3033 - val_acc: 0.7928\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import makedirs\n# makedirs('models')\n# fit and save models\nfilename = 'models/model_caps.h5'\nmodel_caps.save(filename)\nprint('>Saved %s' % filename)\n# filename = 'models/model_densenet.h5'\n# model_densenet.save(filename)\n# print('>Saved %s' % filename)\nfilename = 'models/model_VGG19.h5'\nmodel_vgg19.save(filename)\nprint('>Saved %s' % filename)\nfilename = 'models/model_resnext.h5'\nmodel_resnext.save(filename)\nprint('>Saved %s' % filename)","execution_count":43,"outputs":[{"output_type":"stream","text":">Saved models/model_caps.h5\n>Saved models/model_VGG19.h5\n>Saved models/model_resnext.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_caps.evaluate(x_test, y_test, verbose = 0))\nmodel_caps.metrics_names","execution_count":44,"outputs":[{"output_type":"stream","text":"[0.161274959105759, 0.4048856068009566, 0.685419596951722]\n","name":"stdout"},{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"['loss', 'mean_absolute_error', 'acc']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_vgg19.evaluate(x_test, y_test, verbose = 0))\n# model_caps.metrics_names","execution_count":47,"outputs":[{"output_type":"stream","text":"[0.6380788225329542, 0.4608201983944106, 0.6788560714008548]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_resnext.evaluate(x_test, y_test, verbose = 0))\nmodel_resnext.metrics_names","execution_count":46,"outputs":[{"output_type":"stream","text":"[20.036126340212423, 0.32114393142847963, 0.6788560714008548]\n","name":"stdout"},{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"['loss', 'mean_absolute_error', 'acc']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_densenet.evaluate(x_test, y_test, verbose = 0))\nmodel_densenet.metrics_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import load_model\nmembers = []\n# filename = 'models/model_caps.h5'\n# model = load_model(filename)\nmembers.append(model_caps)\n# filename = 'models/model_VGG19.h5'\n# model = load_model(filename)\nmembers.append(model_vgg19)\n# del model\n# members.append(model_densenet)\nmembers.append(model_resnext)\nmembers","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"[<keras.engine.training.Model at 0x7f27d0c9c358>,\n <keras.engine.sequential.Sequential at 0x7f2d281d0ac8>,\n <keras.engine.sequential.Sequential at 0x7f2d28183e48>]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in members:\n    acc = model.evaluate(x_test, y_test, verbose=0)[-1]\n    print('Model Accuracy: %.3f' % acc)\nmodel_vgg19.predict(x_test, verbose=0).shape","execution_count":49,"outputs":[{"output_type":"stream","text":"Model Accuracy: 0.685\nModel Accuracy: 0.679\nModel Accuracy: 0.679\n","name":"stdout"},{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"(2133, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\n# create stacked model input dataset as outputs from the ensemble\ndef stacked_dataset(members, inputX):\n    stackX = None\n    for model in members:\n        # make prediction\n        yhat = model.predict(inputX, verbose=0)\n        # stack predictions into [rows, members, probabilities]\n        if stackX is None:\n            stackX = yhat\n        else:\n            stackX = np.dstack((stackX, yhat))\n    # flatten predictions to [rows, members x probabilities]\n    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n    return stackX\n\n# fit a model based on the outputs from the ensemble members\ndef fit_stacked_model(members, inputX, inputy):\n    # create dataset using ensemble\n    stackedX = stacked_dataset(members, inputX)\n    # fit standalone model\n    model = LogisticRegression()\n    model.fit(stackedX, inputy)\n    return model\n\n# fit stacked model using the ensemble\nmodel = fit_stacked_model(members, x_test, y_dopetest)\n\n# make a prediction with the stacked model\ndef stacked_prediction(members, model, inputX):\n    # create dataset using ensemble\n    stackedX = stacked_dataset(members, inputX)\n    # make a prediction\n    yhat = model.predict(stackedX)\n    return yhat\n\n# evaluate model on test set\nyhat = stacked_prediction(members, model, x_test)\nacc = accuracy_score(y_dopetest, yhat)\nprint('Stacked Test Accuracy: %.3f' % acc)","execution_count":50,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"Stacked Test Accuracy: 0.797\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.applications.resnet import ResNet152\n# model_dop = ResNet152(include_top=False, weights='imagenet', pooling=\"max\")\n# model_dop.add(Dense(num_classes, activation = \"softmax\"))\n# model_dop.layers[0].trainable = False\n# model_dop.compile(optimizer= \"Adamax\", loss='categorical_crossentropy', metrics=['mae', 'accuracy'])\n# print(\"*\"*40 + \"ResNeXt101 model\" + \"*\"*40)\n# history = model_dop.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n#                            verbose = 1,\n#                            epochs = epochs,\n#                            steps_per_epoch=x_train.shape[0] // batch_size,\n#                                      callbacks = [learning_rate_reduction],\n#                            validation_data = (x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset Reference :- [CodaLab_competition](https://competitions.codalab.org/competitions/20429#learn_the_details-terms_and_conditions)"},{"metadata":{},"cell_type":"markdown","source":"| Model/Architecture | Type | Optimizer | Accuracy(in %) |\n| --- | --- | --- | --- |\n| CNN | Raw | Adam | ~65 ||\n| ResNet50 | Pre-Trained | sgd | ~64 |\n| VGG16 | Pre-Trained | sgd | ~77 |\n| VGG19 | Pre-Trained | sgd | ~79 |\n| Inception | Pre-Trained | sgd | ~64 |\n| VGG16 | Pre-Trained | Adam | ~81 |\n| VGG19 | Pre-Trained | Adam | ~83 |\n| VGG16 | Pre-Trained | Adamax | ~79 |\n| VGG19 | Pre-Trained | Adamax | ~78 |\n| VGG16 | Pre-Trained | Nadam | ~80 |\n| VGG19 | Pre-Trained | Nadam | ~79 |"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}